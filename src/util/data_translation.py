import logging
import random
import os
import pickle
import math
import time
import nltk
import html
import copy
import itertools
import json
import string

from googleapiclient.discovery import build
from src import config
from src.util import data_processing
from src.util.annotation_evaluation import post_process_annotations
from datetime import datetime
from collections import Counter
from nltk.corpus import stopwords

logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)
logging.getLogger('googleapiclient.discovery').setLevel(logging.ERROR)
logging.getLogger('cloudpickle').setLevel(logging.ERROR)

logging.basicConfig(level=logging.DEBUG, filename="translation.log")

logger = logging.getLogger()

handler = logging.FileHandler("translation_en-de.log")
handler.setLevel(logging.DEBUG)
logger.addHandler(handler)

random.seed(config.SEED)

MATCHING_SCORE_THRESHOLD = 0.50
MIN_LEN = 3
MAX_SET_SIZE = 1000
MAX_BORDER_LEN = 3
TF_IDF_THRESHOLD = 0.50
MOST_COMMON = 10

DATE_TODAY = datetime.today().strftime("%d-%m-%Y")
# DATE_TODAY = "25-11-2018"
print("Today's date: ", DATE_TODAY)


class Translation:
    def __init__(self, annotated_list, args):
        self.src_annotated_list = annotated_list
        self.args = args
        self.base_path = os.path.join(args.data_path, args.src_lang + "-" + args.tgt_lang)
        self.sentence_ids = None
        self.tgt_sentence_list = None
        self.tgt_phrase_list = None
        self.src_phrase_list = None
        self.tgt_annotated_list = list()
        self.tgt_phrase_instance_list = list()
        self.suffix = None
        self.drop_list = None
        self.tgt_lang = args.tgt_lang
        self.epi = config.get_epi(self.tgt_lang)
        self.stop_word_list = set(stopwords.words(config.stop_word_dict[self.tgt_lang]))

    def translate_data(self):
        num_sentences = len(self.src_annotated_list)

        if self.args.translate_all == 0:
            self.suffix = str(self.args.num_sample)
        else:
            self.suffix = "all"

        if self.args.translate_all == 0:
            if self.args.num_sample > num_sentences:
                raise ValueError("Cannot sample more than the number of sentences.")
            sentence_ids_file_name = self.args.sentence_ids_file + "_" + self.suffix + ".pkl"
            sentence_ids_path = os.path.join(self.base_path, sentence_ids_file_name)
            if os.path.exists(sentence_ids_path):
                sentence_ids = pickle.load(open(sentence_ids_path, 'rb'))
            else:
                sentence_ids = random.sample(list(range(num_sentences)), self.args.num_sample)
                with open(sentence_ids_path, 'wb') as f:
                    pickle.dump(sentence_ids, f)
        else:
            sentence_ids = list(range(num_sentences))

        src_sentence_list = [' '.join(self.src_annotated_list[sid].tokens) for sid in sentence_ids]

        if self.args.verbosity == 2:
            print("Source sentence list loaded. Length: %d" % len(src_sentence_list))

        # File names to either read from (if trans_sent is 0) or write to (if trans_sent is 1)
        tgt_sentence_path = os.path.join(self.base_path, self.args.translate_fname +
                                         '_' + self.suffix + config.PKL_EXT)
        tgt_phrase_path = os.path.join(self.base_path, self.args.translate_fname +
                                       '_tgt_phrases_' + self.suffix + config.PKL_EXT)
        src_phrase_path = os.path.join(self.base_path, self.args.translate_fname +
                                       '_src_phrases_' + self.suffix + config.PKL_EXT)

        if self.args.trans_sent == 2:
            # Get sentence list in target language
            batch_size = config.BATCH_SIZE
            tgt_sentence_list = _get_saved_list(tgt_sentence_path)
            print(len(tgt_sentence_list))

            max_iter = int(math.ceil(num_sentences/batch_size))
            for i in range(max_iter):

                beg = i * batch_size
                end = (i + 1) * batch_size

                if self.args.verbosity == 2:
                    print("Batch ID: %d" % i)
                    print("From %d to %d..." % (beg, end))

                src_sentences = src_sentence_list[beg:end]
                tgt_sentence_list.extend(get_google_translations(src_sentences,
                                                                 self.args.src_lang,
                                                                 self.args.tgt_lang,
                                                                 self.args.api_key))
                with open(tgt_sentence_path, 'wb') as f:
                    pickle.dump(tgt_sentence_list, f)
                time.sleep(10)

        if self.args.trans_sent >= 1:
            tgt_sentence_list = pickle.load(open(tgt_sentence_path, 'rb'))

            src_phrase_list = list()
            print("Length of source phrase list: ", len(src_phrase_list))
            tgt_phrase_list = list()
            print("Length of target phrase list: ", len(tgt_phrase_list))

            for i, sid in enumerate(sentence_ids):
                if self.args.verbosity >= 1:
                    print("######################################################################")
                    print("Sentence-%d" % i)

                a = self.src_annotated_list[sid]
                span_list = a.span_list

                tgt_tokens = get_clean_tokens(tgt_sentence_list[i])
                if self.args.verbosity == 2:
                    print("Source tokens: ", a.tokens)
                    print("Tgt tokens: ", tgt_tokens)

                src_phrase_list.append(list())
                for span in span_list:
                    phrase = ' '.join(a.tokens[span.beg:span.end])
                    src_phrase_list[i].append(phrase)
                    if self.args.verbosity == 2:
                        print("Beginning: %d, End: %d, Tag: %s" % (span.beg, span.end,
                                                                   span.tag_type), end='')
                        print(", Phrase: ", phrase)

                if self.args.verbosity == 2:
                    print("Source phrase list: ", src_phrase_list[i])

                if src_phrase_list[i] != list():
                    tgt_phrase_list.append(get_google_translations(src_phrase_list[i], self.args.src_lang, self.args.tgt_lang, self.args.api_key))
                else:
                    tgt_phrase_list.append(list())

                with open(tgt_phrase_path, 'wb') as f:
                    pickle.dump(tgt_phrase_list, f)
                with open(src_phrase_path, 'wb') as f:
                    pickle.dump(src_phrase_list, f)

                    if self.args.verbosity == 2:
                        print("Target phrase list: ", tgt_phrase_list[i])

            if self.args.verbosity >= 1:
                print("######################################################################")
                print("Source entities (phrases) translated.")

            with open(tgt_phrase_path, 'wb') as f:
                pickle.dump(tgt_phrase_list, f)
            with open(src_phrase_path, 'wb') as f:
                pickle.dump(src_phrase_list, f)

        else:
            tgt_sentence_list = pickle.load(open(tgt_sentence_path, 'rb'))
            tgt_phrase_list = pickle.load(open(tgt_phrase_path, 'rb'))
            src_phrase_list = pickle.load(open(src_phrase_path, 'rb'))

        self.sentence_ids = sentence_ids
        self.tgt_sentence_list = tgt_sentence_list
        self.tgt_phrase_list = tgt_phrase_list
        self.src_phrase_list = src_phrase_list

        for i, src_phrase in enumerate(self.src_phrase_list):
            assert len(src_phrase) == len(self.tgt_phrase_list[i])

        # for i, sent in enumerate(tgt_sentence_list):
        #     tokens = sent.split(" ")
        #     if len(tokens) >= 3:
        #         if tokens[2] == "internacionales":
        #             print(str(i) + ": " + sent)

        if self.args.verbosity >= 1:
            print("######################################################################")
            print("Target sentence list loaded. Length: %d" % len(tgt_sentence_list))
            print("Source entity (phrase) list loaded. Length: %d" % len(src_phrase_list))
            print("Target entity (phrase) list loaded. Length: %d" % len(tgt_phrase_list))

    def prepare_mega_tgt_phrase_list(self, calc_count_found=False):
        lexicons = list()
        tgt_phrase_instance_list = list()

        tgt_phrase_instance_list_path = os.path.join(self.base_path, "tgt_phrase_instance_list.pkl")
        if os.path.exists(tgt_phrase_instance_list_path):
            self.tgt_phrase_instance_list = pickle.load(open(tgt_phrase_instance_list_path, 'rb'))

        else:
            for lexicon_file_name in config.LEXICON_FILE_NAMES:
                lexicon = pickle.load(open(os.path.join(self.base_path, lexicon_file_name + ".pkl"), "rb"))
                lexicons.append(lexicon)

            for i, src_phrase_sent in enumerate(self.src_phrase_list):
                tgt_phrase_instance_list.append(list())
                for j, src_phrase in enumerate(src_phrase_sent):
                    tgt_phrase_instance = TgtPhrases(src_phrase)
                    tgt_phrase_instance.add_tgt_phrase([self.tgt_phrase_list[i][j].replace("&#39;", "\'")])
                    tgt_phrase_instance.add_tgt_phrase([src_phrase])
                    tgt_phrase_instance_list[i].append(tgt_phrase_instance)

                for k, lexicon in enumerate(lexicons):
                    for l, src_phrase in enumerate(src_phrase_sent):
                        temp = dict()
                        tgt_phrase = list()

                        src_phrase = src_phrase.split(" ")
                        for m, src_token in enumerate(src_phrase):
                            temp[m] = list()
                            if src_token.lower() in lexicon:
                                for tgt_token in lexicon[src_token.lower()]:
                                    temp[m].append(tgt_token.capitalize())
                            else:
                                temp[m].append(src_token)

                        for key in temp:
                            if tgt_phrase == list():
                                tgt_phrase = temp[key]
                            else:
                                tgt_phrase = [phrase_1 + " " + phrase_2 for phrase_1 in tgt_phrase
                                              for phrase_2 in temp[key]]

                        tgt_phrase_instance_list[i][l].add_tgt_phrase(tgt_phrase)

            with open(tgt_phrase_instance_list_path, "wb") as f:
                pickle.dump(tgt_phrase_instance_list, f)

            self.tgt_phrase_instance_list = tgt_phrase_instance_list

            if calc_count_found:
                total_tokens = list()
                translations_found = list()
                translation_tokens = list()
                all_tokens = list()

                for _ in config.LEXICON_FILE_NAMES:
                    total_tokens.append(0)
                    translations_found.append(0)
                    translation_tokens.append(list())

                for i, src_phrase_sent in enumerate(self.src_phrase_list):
                    for k, lexicon in enumerate(lexicons):
                        for l, src_phrase in enumerate(src_phrase_sent):
                            src_phrase = src_phrase.split(" ")
                            for m, src_token in enumerate(src_phrase):
                                total_tokens[k] += 1
                                all_tokens.append(src_token)
                                if src_token.lower() in lexicon:
                                    translation_tokens[k].append(src_token)
                                    translations_found[k] += 1

                for i, _ in enumerate(lexicons):
                    print("Unique tokens in IDC: ", len(set(translation_tokens[1])))
                    print("Unique tokens in ground truth: ", len(set(translation_tokens[0])))
                    print("Unique tokens in source entities: ", len(set(all_tokens)))
                    print("In IDC but not in ground truth: ", len(set(translation_tokens[1]).difference(set(translation_tokens[0]))))
                    print("In ground truth but not in IDC: ", len(set(translation_tokens[0]).difference(set(translation_tokens[1]))))
                    print("Total tokens: ", total_tokens[i])
                    print("Translations found: ", translations_found[i])
                    print("Fraction of translations found: %.4f" % float(translations_found[i] / total_tokens[i]))

        self._test_tgt_phrase_instance_list()

    def _test_tgt_phrase_instance_list(self):
        for tgt_phrase_instances in self.tgt_phrase_instance_list:
            for tpinstance in tgt_phrase_instances:
                assert len(tpinstance.tgt_phrase_list) == len(config.LEXICON_FILE_NAMES) + 2

    def get_tgt_annotations_new(self):
        # Target language tokens (obtained through Google Translate + cleaned)
        path = os.path.join(self.base_path, self.args.translate_fname + "_tgt_annotated_list_period_" + self.suffix + ".pkl")
        if os.path.exists(path):
            self.tgt_annotated_list = pickle.load(open(path, "rb"))

        else:
            for i, sentence in enumerate(self.tgt_sentence_list):
                print("######################################################################")
                print("Sentence-%d: %s" % (i, sentence))
                sentence = sentence.replace("&#39;", "\'")
                sentence = sentence.replace(" &amp; ", "&")
                tgt_annotation = data_processing.Annotation()
                tgt_annotation.tokens = get_clean_tokens(sentence)
                print("Tokens: ", tgt_annotation.tokens)
                self.tgt_annotated_list.append(tgt_annotation)
            with open(path, "wb") as f:
                pickle.dump(self.tgt_annotated_list, f)

        path = os.path.join(self.base_path, self.args.translate_fname +
                            "_annotated_list_0.5_period_" + self.suffix + DATE_TODAY + "_partial" + ".pkl")

        if os.path.exists(path):
            self.tgt_annotated_list = pickle.load(open(path, "rb"))

        else:
            candidates_list = self.get_candidates_list()
            potential_matches = self.get_potential_matches(candidates_list)
            self.get_partial_tags(potential_matches)
            with open(path, 'wb') as f:
                pickle.dump(self.tgt_annotated_list, f)

        path = os.path.join(self.base_path, self.args.translate_fname +
                            "_annotated_list_0.5_period_" + self.suffix + DATE_TODAY + ".pkl")

        if os.path.exists(path):
            self.tgt_annotated_list = pickle.load(open(path, "rb"))

        else:
            print("Final tags unavailable. Getting them...")
            self.get_final_tags()
            with open(path, 'wb') as f:
                pickle.dump(self.tgt_annotated_list, f)

        # path = os.path.join(self.base_path, "problematic_entities_" + DATE_TODAY + ".pkl")
        # problematic_entities = pickle.load(open(path, "rb"))
        #
        # path = os.path.join(self.base_path, "problem_children_" + DATE_TODAY + ".pkl")
        # problem_children = pickle.load(open(path, "rb"))
        #
        # path = os.path.join(self.base_path, "problematic_sentences_" + DATE_TODAY + ".pkl")
        # problematic_sentences = pickle.load(open(path, "rb"))
        #
        # final_candidate_dict = self.get_refined_candidates(problem_children, problematic_entities)
        # self.tag_problematic_sentences(problematic_sentences, final_candidate_dict, problematic_entities)

    def get_candidates_list(self):
        candidates_list = list()
        # self.sentence_ids = [7963]
        # self.sentence_ids = list(range(100))

        print("######################################################################")
        print("######################################################################")
        print("Step-1: Getting candidate tags.")
        print("######################################################################")
        print("######################################################################")
        for i, sid in enumerate(self.sentence_ids):
            src_a = self.src_annotated_list[sid]
            src_phrases = self.src_phrase_list[sid]

            print("######################################################################")
            print("Sentence ID: ", sid)
            print("######################################################################")

            candidates_list.append(list())
            if src_phrases != list():
                for j, src_phrase in enumerate(src_phrases):
                    print("######################################################################")
                    print("Source phrase: ", src_phrase)
                    print("######################################################################")

                    candidates_list[i].append(list())
                    all_tgt_phrases = self.tgt_phrase_instance_list[sid][j].tgt_phrase_list

                    for k, tgt_phrases in enumerate(all_tgt_phrases):
                        candidates_list[i][j].append(list())
                        for l, tgt_phrase in enumerate(tgt_phrases):
                            if l == 100:
                                break
                            tag_type = src_a.span_list[j].tag_type
                            tgt_tokens = tgt_phrase.split(" ")

                            candidates_list[i][j][k].append(list())
                            for m, tgt_token in enumerate(tgt_tokens):
                                if m == 0:
                                    ner_tag = "B-" + tag_type
                                else:
                                    ner_tag = "I-" + tag_type
                                print("Target token: ", tgt_token)
                                candidates_list[i][j][k][l].append((tgt_token, ner_tag))
        return candidates_list

    def get_potential_matches(self, candidates_list):
        print("######################################################################")
        print("######################################################################")
        print("Step-2: Getting all possible potential matches.")
        print("######################################################################")
        print("######################################################################")
        potential_matches = list()
        for i, sid in enumerate(self.sentence_ids):
            tgt_matches = dict()
            sent_candidates = candidates_list[i]
            if sent_candidates == list():
                print("%d: " % i, sent_candidates)
            else:
                for j, phrase_candidates in enumerate(sent_candidates):
                    for k, diff_phrase_candidates in enumerate(phrase_candidates):
                        for l, all_candidates in enumerate(diff_phrase_candidates):
                            for m, candidate in enumerate(all_candidates):
                                print("%d.%d.%d.%d.%d Candidate token: %s" % (i, j, k, l, m, candidate[0]), end='')
                                print(", Candidate tag: ", candidate[1])

                                tgt_a = self.tgt_annotated_list[sid]

                                temp_matches = dict()
                                for o, matching_algo in enumerate(config.MATCHING_ALGOS):
                                    temp_matches[matching_algo] = list()

                                for n, reference_token in enumerate(tgt_a.tokens):
                                    if n not in tgt_matches:
                                        tgt_matches[n] = set()
                                    for o, matching_algo in enumerate(config.MATCHING_ALGOS):
                                        if o == 0:
                                            transliterate = False
                                        else:
                                            transliterate = True
                                        x = _find_match(reference_token, candidate[0], self.epi,
                                                        self.stop_word_list, transliterate=transliterate)
                                        temp_matches[matching_algo].append(x)

                                for o, matching_algo in enumerate(config.MATCHING_ALGOS):
                                    temp_match = temp_matches[matching_algo]
                                    temp_match = list(map(list, zip(*temp_match)))
                                    if True in temp_match[1]:
                                        max_score = max(temp_match[0])
                                        max_indices = [ind for ind, score in enumerate(temp_match[0])
                                                       if score == max_score]
                                        for index in max_indices:
                                            if "-" in tgt_a.tokens[index]:
                                                tgt_matches[index].add((j, k, o, max_score, candidate[1]))
                                            elif max_score >= MATCHING_SCORE_THRESHOLD:
                                                tgt_matches[index].add((j, k, o, max_score, candidate[1]))

            potential_matches.append(tgt_matches)
        return potential_matches

    def get_partial_tags(self, potential_matches):
        print("######################################################################")
        print("######################################################################")
        print("Step-3: Getting a minimal set of potential matches.")
        print("######################################################################")
        print("######################################################################")
        for i, sid in enumerate(self.sentence_ids):
            potential_match_sent = potential_matches[i]
            tgt_a = self.tgt_annotated_list[sid]
            print("######################################################################")
            print("Sentence ID: ", sid)
            print("######################################################################")

            if potential_match_sent == dict():
                tgt_a.ner_tags = [config.OUTSIDE_TAG for _ in tgt_a.tokens]

            for key, val in potential_match_sent.items():
                print("Key: ", tgt_a.tokens[key], " Value: ", val)
                val = list(val)
                all_vals = list(map(list, zip(*val)))
                if all_vals == list():
                    tgt_a.ner_tags.append(config.OUTSIDE_TAG)
                else:
                    val.sort(key=lambda a: (-a[3], a[1], a[2]))

                    phrases = set(all_vals[0])
                    tags = set(all_vals[-1])

                    new_vals = list()
                    phrase_added = list()
                    tag_added = list()
                    ind_added = list()

                    for t, v in enumerate(val):
                        for phrase in set(phrases):
                            if (phrase not in phrase_added) and (v[0] == phrase):
                                new_vals.append(v)
                                phrase_added.append(phrase)
                                ind_added.append(t)

                        for tag in set(tags):
                            if (tag not in tag_added) and (t not in ind_added) and (v[-1] == tag):
                                new_vals.append(v)
                                tag_added.append(tag)
                                ind_added.append(t)

                    tgt_a.ner_tags.append(new_vals)
                    print("Final tag: ", new_vals)

    def get_final_tags(self):
        print("######################################################################")
        print("######################################################################")
        print("Step-4: Getting final annotations.")
        print("######################################################################")
        print("######################################################################")
        problematic_entities = Counter()
        src_entities = 0
        occur_problem_entities = 0
        problematic_sentences = dict()
        problem_children = dict()
        idf = dict()

        for i, sid in enumerate(self.sentence_ids):
            print("######################################################################")
            print("Sentence ID: ", sid)
            print("######################################################################")
            tgt_a = self.tgt_annotated_list[sid]
            src_a = self.src_annotated_list[sid]
            src_phrases = self.src_phrase_list[sid]
            ner_tag_list = tgt_a.ner_tags

            dummy_list = [-1 for _ in ner_tag_list]

            entity_candidates = {j: copy.deepcopy(dummy_list) for j in range(len(src_phrases))}
            find_entity_spans(entity_candidates, ner_tag_list)

            score_match = dict()
            for j in entity_candidates:
                src_entities += 1
                span_list = get_spans(j, entity_candidates[j])

                for tok in set(tgt_a.tokens):
                    if tok not in idf:
                        idf[tok] = 0
                    idf[tok] += 1

                if span_list == list():
                    occur_problem_entities += 1
                    if i not in problematic_sentences:
                        problematic_sentences[i] = list()
                    problematic_sentences[i].append((j, src_phrases[j]))

                    problematic_entities.update({src_phrases[j]: 1})
                    logging.info("######################################################################")
                    logging.info("No correspondence found in sentence: " + str(sid))
                    logging.info("Source tokens: " + str(src_a.tokens))
                    logging.info("Target tokens: " + str(tgt_a.tokens))
                    logging.info("Source phrase for which no match found: " + str(src_phrases[j]))

                    if src_phrases[j] not in problem_children:
                        problem_children[src_phrases[j]] = Counter()

                    for tok in tgt_a.tokens:
                        problem_children[src_phrases[j]].update({tok: 1})

                    logging.info("######################################################################")

                else:
                    print("######################################################################")
                    print("Source phrase: ", src_phrases[j])
                    print("######################################################################")
                    temp_tgt_phrases = set()
                    all_tgt_phrases = self.tgt_phrase_instance_list[sid][j].tgt_phrase_list
                    for k, candidate_phrases in enumerate(all_tgt_phrases):
                        temp_tgt_phrases.update(candidate_phrases)

                    final_tgt_phrases = copy.deepcopy(temp_tgt_phrases)
                    for tgt_phrase in temp_tgt_phrases:
                        if len(final_tgt_phrases) > MAX_SET_SIZE:
                            break
                        if tgt_phrase != all_tgt_phrases[0][0]:
                            permutations = itertools.permutations(tgt_phrase.split(" "))
                            final_tgt_phrases.update([" ".join(x) for x in permutations])

                    if len(final_tgt_phrases) > 2 * MAX_SET_SIZE:
                        final_tgt_phrases = copy.deepcopy(temp_tgt_phrases)

                    print("All source candidate phrases: ", len(final_tgt_phrases))

                    new_span_list = list()
                    for l, span in enumerate(span_list):
                        start, end = span
                        orig_start = start
                        orig_end = end

                        start_list = [orig_start]
                        while tgt_a.tokens[start] in self.stop_word_list:
                            start += 1
                            if start == orig_end:
                                break
                            start_list.append(start)

                        end_list = [orig_end]
                        while tgt_a.tokens[end-1] in self.stop_word_list:
                            end -= 1
                            if end == orig_start:
                                break
                            end_list.append(end)

                        for s in start_list:
                            for e in end_list:
                                new_span_list.append((s, e))

                    best_score = float("inf")

                    for l, span in enumerate(new_span_list):
                        tgt_phrase = " ".join(tgt_a.tokens[span[0]:span[1]])

                        print("Target candidate phrase: ", tgt_phrase)

                        score = float("inf")
                        best_possible_translation = ""

                        if len(final_tgt_phrases) < 100:
                            print("Final target phrases: ", final_tgt_phrases)
                        else:
                            print("Size of target phrase set more than 100.")

                        for possible_translation in final_tgt_phrases:
                            edit_dist = nltk.edit_distance(tgt_phrase.lower(), possible_translation.lower())
                            score = min(score, edit_dist)
                            if score == edit_dist:
                                best_possible_translation = possible_translation

                        print("Score: ", score)

                        best_score = min(best_score, score)
                        if best_score == score:
                            if span not in score_match:
                                score_match[span] = dict()
                            score_match[span][j] = (best_score, best_possible_translation)

                    for span in score_match:
                        if j in score_match[span]:
                            if score_match[span][j][0] > best_score:
                                score_match[span].pop(j)

            print("Final score match: ", score_match)
            for span in score_match:
                if score_match[span] != dict():
                    max_score = max([v[0] for v in score_match[span].values()])
                    max_indices = [i for i in score_match[span].keys()
                                   if score_match[span][i][0] == max_score]

                    # Pick the first one.
                    tag_type = src_a.span_list[max_indices[0]].tag_type
                    start, end = span
                    best_span = list(range(start, end))
                    for ind, sp in enumerate(best_span):
                        if ind == 0:
                            prefix = "B"
                        else:
                            prefix = "I"
                        tgt_a.ner_tags[sp] = prefix + "-" + tag_type

            print("######################################################################")
            print("Target tokens: ", tgt_a.tokens)
            for ind, tag in enumerate(tgt_a.ner_tags):
                if tag not in config.allowed_tags:
                    tgt_a.ner_tags[ind] = config.OUTSIDE_TAG
            print("Target NER tags: ", tgt_a.ner_tags)

        logging.info("######################################################################")
        logging.info("Entities with no match: " + str(problematic_entities))
        logging.info("Number of unique entities with no match: " + str(len(problematic_entities)))
        logging.info("Number of occurrences of entities with no match: " + str(occur_problem_entities))
        logging.info("Number of all source entities: " + str(src_entities))

        N = len(self.sentence_ids)
        logging.info("Total documents: " + str(N))
        logging.info("Potential candidates for problematic entities:")
        count_pc = 0
        for pc in problem_children:
            if problematic_entities[pc] > 1:
                count_pc += 1
                for k, v in problem_children[pc].items():
                    assert k in idf
                    problem_children[pc][k] = v * math.log(N/idf[k])
                logging.info("######################################################################")
                logging.info(pc + ": " + str(problem_children[pc].most_common(100)))

        sentences_to_drop = list()
        for k, val in problematic_sentences.items():
            to_be_dropped = False
            for v in val:
                if problematic_entities[v[1]] > 1:
                    to_be_dropped = True
            if to_be_dropped:
                sentences_to_drop.append(k)

        logging.info("Number of problematic entities with count > 1 (PC-1): " + str(count_pc))
        logging.info("Number of sentences with at least on PC-1 entity: " + str(len(sentences_to_drop)))

        # # self.drop_list = sentences_to_drop

        path = os.path.join(self.base_path, "problematic_entities_" + DATE_TODAY + ".pkl")
        with open(path, 'wb') as f:
            pickle.dump(problematic_entities, f)

        path = os.path.join(self.base_path, "problem_children_" + DATE_TODAY + ".pkl")
        with open(path, 'wb') as f:
            pickle.dump(problem_children, f)

        path = os.path.join(self.base_path, "problematic_sentences_" + DATE_TODAY + ".pkl")
        with open(path, 'wb') as f:
            pickle.dump(problematic_sentences, f)

        final_candidate_dict = self.get_refined_candidates(problem_children, problematic_entities)
        self.tag_problematic_sentences(problematic_sentences, final_candidate_dict, problematic_entities)

    def get_refined_candidates(self, problem_children, problematic_entities):
        final_candidate_dict = dict()
        for token in problem_children:
            if problematic_entities[token] > 1:
                most_common_candidates = problem_children[token].most_common(MOST_COMMON)
                max_score = most_common_candidates[0][1]

                final_candidates = []

                for i, candidates in enumerate(most_common_candidates):
                    if candidates[0] not in self.stop_word_list and \
                            candidates[0] not in string.punctuation:
                        if candidates[1] >= TF_IDF_THRESHOLD * max_score:
                            final_candidates.append(candidates)
                        else:
                            break

                final_candidate_dict[token] = final_candidates
                logging.info("######################################################################")
                logging.info("Token: " + str(token) + " Final candidates: " + str(final_candidates))
        logging.info("######################################################################")
        logging.info("Final candidates dict: " + str(final_candidate_dict))
        return final_candidate_dict

    def tag_problematic_sentences(self, problematic_sentences, final_candidate_dict, problematic_entities):
        total_entities = 0
        total_annotations_done = 0

        logging.info("Tagging problematic sentences.")
        for sent in problematic_sentences:
            logging.info("######################################################################")
            logging.info("Sentence id: " + sent)
            src_a = self.src_annotated_list[sent]
            tgt_a = self.tgt_annotated_list[sent]
            logging.info("Source tokens: " + str(src_a.tokens))
            logging.info("Target tokens: " + str(tgt_a.tokens))

            for entities in problematic_sentences[sent]:
                if problematic_entities[entities[1]] > 1:
                    total_entities += 1
                    tag_type = src_a.span_list[entities[0]].tag_type
                    entity_str = entities[1]

                    candidate_list = final_candidate_dict[entity_str]
                    token_list = {k: v for k, v in candidate_list}
                    temp_ner_list = [(-1, 0) for _ in tgt_a.tokens]

                    for i, tgt_token in enumerate(tgt_a.tokens):
                        if tgt_token in token_list:
                            temp_ner_list[i] = (tag_type, token_list[tgt_token])

                    logging.info("######################################################################")
                    logging.info("Entity: " + str(entity_str))
                    ordered_span_list = get_ordered_spans(tag_type, temp_ner_list)
                    logging.info("Ordered span list: " + str(ordered_span_list))

                    for span in ordered_span_list:
                        if self.can_tag(span[0], span[1], tgt_a):
                            logging.info("Tagging possible!")
                            logging.info("Target phrase: " + " ".join(tgt_a.tokens[span[0]:span[1]]))
                            logging.info("Tag: " + tag_type)
                            total_annotations_done += 1
                            best_span = list(range(span[0], span[1]))
                            for ind, sp in enumerate(best_span):
                                if ind == 0:
                                    prefix = "B"
                                else:
                                    prefix = "I"
                                tgt_a.ner_tags[sp] = prefix + "-" + tag_type
                            break

        logging.info("######################################################################")
        logging.info("Number of PC-1 entities: " + str(total_entities))
        logging.info("Number of annotated entities: " + str(total_annotations_done))

    def can_tag(self, span_beg, span_end, tgt_a):
        for i in range(span_beg, span_end):
            if tgt_a.ner_tags[i] != "O":
                if type(tgt_a.ner_tags[i]) != list:
                    return False
        return True

    def prepare_train_file(self):
        self.tgt_annotated_list = post_process_annotations(self.tgt_annotated_list, self.stop_word_list)
        file_path = os.path.join(self.base_path, self.args.translate_fname + "_affix-match_0.5_period_" + DATE_TODAY)
        data_processing.prepare_train_file(self.tgt_annotated_list, self.drop_list, file_path)


class TgtPhrases:
    def __init__(self, src_phrase):
        self.src_phrase = src_phrase
        self.tgt_phrase_list = list()

    def add_tgt_phrase(self, tgt_phrase):
        self.tgt_phrase_list.append(tgt_phrase)


class Match:
    def __init__(self, token, ner_tag):
        self.token = token
        self.ner_tag = ner_tag
        self.match_list = list()

    def add_match(self, ref_id, score):
        self.match_list.append((ref_id, score))


def get_google_translations(src_sentence_list, src_lang, tgt_lang, api_key):
    service = build('translate', 'v2', developerKey=api_key)
    tgt_dict = service.translations().list(source=src_lang, target=tgt_lang, q=src_sentence_list).execute()
    tgt_sentence_list = [t['translatedText'] for t in tgt_dict['translations']]
    return tgt_sentence_list


def get_clean_tokens(sentence, print_info=False):
    for entity in html.entities.html5:
        sentence = sentence.replace("&" + entity + ";", html.entities.html5[entity])

    tokens = nltk.word_tokenize(sentence)
    # if print_info:
    #     print("Sentence: ", sentence)
    #     print("Tokens: ", tokens)

    final_tokens = list()
    ampersand_found = False
    prior_period_index = -float("inf")
    for i, token in enumerate(tokens):
        if ampersand_found:
            ampersand_found = False
            continue

        elif token == "&" and i != 0 and i != len(tokens)-1:
            final_tokens.pop()
            final_tokens.append(tokens[i-1] + "&" + tokens[i+1])
            ampersand_found = True

        elif token == "." and i != 0 and i != len(tokens) - 1:
            if prior_period_index == -float("inf") or i - prior_period_index > 2:
                prior_period_index = i
                final_tokens.pop()
                final_tokens.append(tokens[i - 1] + ".")
            else:
                final_tokens.pop()
                final_tokens.pop()
                final_tokens.append(tokens[i - 3] + "." + tokens[i - 1] + ".")

        else:
            if token == "``" or token == "''":
                final_tokens.append("\"")
            else:
                final_tokens.append(token)
    return final_tokens


def _find_match(reference, hypothesis, epi, stop_word_list, transliterate=False):
    reference = copy.deepcopy(reference).lower()
    hypothesis = copy.deepcopy(hypothesis).lower()

    # print("Reference: ", reference, "Hypothesis: ", hypothesis)

    if transliterate:
        hypothesis = epi.transliterate(hypothesis)
        reference = epi.transliterate(reference)

    is_stop_word = False
    if (hypothesis in stop_word_list) or (reference in stop_word_list):
        is_stop_word = True

    L = len(hypothesis)
    score = L
    ret_val = False
    if L == 0 or reference == '':
        return 0, False
    if reference == hypothesis:
        ret_val = True
    else:
        for j in range(L, 0, -1):
            sub_str = hypothesis[:j]
            if reference.startswith(sub_str):
                if not is_stop_word:
                    ret_val = True
                    break
            elif reference.endswith(sub_str):
                if not is_stop_word:
                    ret_val = True
                    break
            score -= 1
    return min(float(score / len(reference)), float(score / L)), ret_val


def generate_fast_align_data(src_sent_list, tgt_sent_list, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        for i, src_sent in enumerate(src_sent_list):
            f.writelines(src_sent + config.FAST_ALIGN_SEP + tgt_sent_list[i] + '\n')


def get_readable_annotations(tokens, annotations):
    # Annotation is a tuple containing score, index, tag
    flat = list(map(list, zip(*annotations)))
    if flat == []:
        full_annotations = [config.OUTSIDE_TAG for _ in tokens]

    else:
        full_annotations = []
        for i, token in enumerate(tokens):
            if i in flat[1]:
                full_annotations.append(flat[2][flat[1].index(i)])
            else:
                full_annotations.append(config.OUTSIDE_TAG)
    return full_annotations


def read_lexicon(src_file_path, tgt_file_path):
    with open(src_file_path, "r", encoding="utf-8") as f:
        rows = f.readlines()
        lexicon = dict()
        for row in rows:
            row = row.rstrip("\n").split(" ")
            src_word = row[0]
            tgt_word = row[1]
            if src_word not in lexicon:
                lexicon[src_word] = list()
            lexicon[src_word].append(tgt_word)

    with open(tgt_file_path, 'wb') as f:
        pickle.dump(lexicon, f)


def _get_saved_list(path):
    if os.path.exists(path):
        saved_list = pickle.load(open(path, 'rb'))
    else:
        saved_list = list()
    return saved_list


def _get_tag_type(tag):
    if tag == config.OUTSIDE_TAG:
        tag_type = None
    else:
        tag_type = tag.split("-")[1]
    return tag_type


def find_entity_spans(entity_candidates, ner_tag_list):
    for i, ner_tags in enumerate(ner_tag_list):
        all_ids = list(map(list, zip(*ner_tags)))

        if all_ids[0] != ["O"]:
            for j in all_ids[0]:
                entity_candidates[j][i] = j


def get_spans(tag_id, tag_list):
    prev_tag = -1
    span_list = list()
    tag_list.append(-1)

    beg = 0
    for i, curr_tag in enumerate(tag_list):
        if (prev_tag == -1) and (curr_tag == tag_id):
            beg = i
        elif (prev_tag == tag_id) and (curr_tag == -1):
            span_list.append((beg, i))
        prev_tag = curr_tag

    return span_list


def get_ordered_spans(tag_id, tag_score_list):
    prev_tag = (-1, 0.0)
    span_list = list()
    tag_score_list.append((-1, 0.0))

    beg = 0
    score = 0
    for i, curr_tag in enumerate(tag_score_list):
        if (prev_tag[0] == -1) and (curr_tag[0] == tag_id):
            beg = i
        elif (prev_tag[0] != -1) and (curr_tag[0] == -1):
            span_list.append((beg, i, score))
            score = 0
        score += curr_tag[1]
        prev_tag = curr_tag

    span_list = sorted(span_list, key=lambda x: x[2], reverse=True)
    return span_list

